{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Linear Disciminant Analysis and Optimization\n",
    "\n",
    "* FDA (2 classes)\n",
    "    * Compute the class means (in PCA space)\n",
    "    * Compute the within-class scatter matrix $\\mathbf{S}_W$ and between-class scatter matrix $\\mathbf{S}_B$\n",
    "    * Compute the projection vector $\\mathbf{w}$\n",
    "    * Compute and plot the 1D projection of the data\n",
    "    * Compute the class separation of the projected values\n",
    "* LDA (3 classes)\n",
    "    * Compute the within-class scatter matrix $\\mathbf{S}_W$ and between-class scatter matrix $\\mathbf{S}_B$\n",
    "    * Compute the projection matrix $\\mathbf{W}$\n",
    "    * Compute and plot the 2D projection of the data\n",
    "    * Compute the 2D LDA projection of the original 64D data \n",
    "* Constrained Optimization\n",
    "    * Reformulate the problem as a maximization problem\n",
    "    * Write out the Lagrangian function\n",
    "    * Compute the gradients with respects to $x_1, x_2$, Lagrange multiplier $\\lambda$ and the KKT multiplier $\\mu$\n",
    "    * Compute the optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.datasets import load_digits, make_swiss_roll\n",
    "from sklearn.decomposition import PCA \n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns; sns.set(); sns.set_palette('bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "As usual, we'll be working with a dataset of handwritten digits. \n",
    "Let visualise some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using a subset of two classes for now\n",
    "digits = load_digits(n_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handy plotting functions\n",
    "\n",
    "def plot_examples():\n",
    "    show_num = 4\n",
    "    _, axes = plt.subplots(1, show_num)\n",
    "    images_and_labels = list(zip(digits.images, digits.target))\n",
    "    for ax, (image, label) in zip(axes[:], images_and_labels[:show_num]):\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        ax.set_title('Label: %i' % label)\n",
    "\n",
    "def plot_scatter(data, target, alpha=0.5, legend=True,w=None):\n",
    "    scatter = plt.scatter(data[:, 0], data[:, 1], c=target, edgecolor='none', alpha=alpha, cmap='rainbow')\n",
    "    if legend:\n",
    "        plt.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Targets\")\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "\n",
    "    if w is not None:\n",
    "        # Define the range of x values based on the scatter plot data\n",
    "        x_values = np.linspace(np.min(data[:, 0]), np.max(data[:, 0]), 100)\n",
    "        \n",
    "        # Calculate corresponding y values based on the line equation: w0 + w1*x1 + w2*x2 = 0 => x2 = -(w0 + w1*x1) / w2\n",
    "        y_values = w[1] * x_values + w[0]\n",
    "\n",
    "        # Plot the line\n",
    "        plt.plot(x_values, y_values, color='red', label='Decision boundary')\n",
    "    #plt.show()\n",
    "    \n",
    "def plot_scatter3d(data, targets, view_point=(25, 45), alpha=0.5, legend=True):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter(data[:,0], data[:,1], data[:,2], c=targets, cmap=\"rainbow\", alpha=alpha)\n",
    "    if legend:\n",
    "        plt.legend(*scatter.legend_elements(), loc=\"upper right\", title=\"Targets\")\n",
    "    ax.view_init(*view_point) # <- change viewpoint here\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get a 2D version of the data using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(digits.data) # this is the representation, we'll be working with\n",
    "t = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot all the data in 2D\n",
    "plot_scatter(X, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Fisher Discriminant Analysis (FDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Compute the class means (in PCA space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Compute the within-class scatter matrix $\\mathbf{S}_W$ and between-class scatter matrix $\\mathbf{S}_B$\n",
    "Use eq (4.27) and (4.28) from textbook for computing $\\mathbf{S}_B$ and $\\mathbf{S}_W$ respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Compute the projection vector $\\mathbf{w}$\n",
    "Hint: Use eq. (4.30) from textbook. You can use `np.linalg.pinv` and `np.linalg.eig` for computing the pseudo-inverse and eigenvectors, respectively. Also, remember to ensure that $||w||_2 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Compute and plot the 1D projection of the data\n",
    "Hint: You can use `seaborn.displot` for a nice visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Compute the class separation of the projected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "n_class = 3\n",
    "digits = load_digits(n_class=n_class)\n",
    "pca = PCA(n_components=3)\n",
    "X = pca.fit_transform(digits.data)\n",
    "t = digits.target\n",
    "\n",
    "plot_scatter3d(X, t, view_point=(35,35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Compute the within-class scatter matrix $\\mathbf{S}_W$ and between-class scatter matrix $\\mathbf{S}_B$\n",
    "See section 4.1.6 in the textbook or Lecture 15 (slide 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Compute the projection matrix $\\mathbf{W}$\n",
    "choose D'=2 (see lecture 15 slide 17 to understand D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Compute and plot the 2D projection of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Compute and plot the 2D LDA projection of the original 64D data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "n_class = 3\n",
    "digits = load_digits(n_class=n_class)\n",
    "X = digits.data\n",
    "t = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute means\n",
    "\n",
    "# compute within class and between-class scatter matrices\n",
    "\n",
    "# Compute eigenvalues and eigenvectors, sort them and select top 2 eigenvectors\n",
    "\n",
    "# perform projection\n",
    "\n",
    "# plot projection in 2D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the general utility of the projection as compared to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Constrained Optimization\n",
    " \n",
    "Consider the problem\n",
    "\n",
    "minimize $f_{min}(x_1, x_2)$\n",
    "\n",
    "subject to $ x_1 + x_2 \\leq 4 \n",
    "\\quad \\text{ and } \\quad x_1 + 4x_2 = 5 \n",
    "$\n",
    "where $f_{min}(x_1, x_2) = (x_1 - 3)^2 + (x_2 - 2)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Reformulate the problem as a canonical maximization problem\n",
    "_Use the form described in the end of \"Pattern Recognition and Machine Learning\" Appendix E._\n",
    "\n",
    "_NB: There is an error in appendix E: $h(x) \\leq 0$, not $h(x) \\geq 0$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Write out the Lagrangian function\n",
    "_Use Equation (E.12) in \"Pattern Recognition and Machine Learning\" Appendix E., and write out all variables_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Compute the gradients with respects to $x_1, x_2$, lagrange multiplier $\\lambda$ and the KKT multiplier $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4) Compute the optimum\n",
    "_Hint: Set it up as a system of linear equations and solve it using Gaussian Elimination (e.g. using `scipy.linalg.solve`)._\n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "1. **Assume the inequality constraint \\( h(x) \\) is inactive.**  \n",
    "   - Set its Lagrange multiplier to zero (\\( \\mu = 0 \\)).  \n",
    "   - Include only the equality constraint \\( g(x) = 0 \\) in your formulation.  \n",
    "   - Construct the system of equations from the KKT conditions:  \n",
    "     \\[\n",
    "     \\nabla f(x) + \\lambda \\nabla g(x) = 0, \\quad g(x) = 0\n",
    "     \\]\n",
    "   - Solve for \\( x_1, x_2, \\lambda \\) using `scipy.linalg.solve`.\n",
    "\n",
    "2. **Check the inequality constraint.**  \n",
    "   - Evaluate \\( h(x_1, x_2) \\).  \n",
    "   - If \\( h(x_1, x_2) < 0 \\), the constraint is inactive â€” keep this as your final solution.\n",
    "\n",
    "3. **If the inequality is violated** (\\( h(x_1, x_2) > 0 \\)):  \n",
    "   - Reformulate the problem assuming \\( h(x) \\) is **active** (\\( h(x) = 0 \\)).  \n",
    "   - Include both constraints in the Lagrangian:  \n",
    "     \\[\n",
    "     \\mathcal{L}(x_1, x_2, \\lambda, \\mu) = f(x_1, x_2) + \\lambda g(x_1, x_2) + \\mu h(x_1, x_2)\n",
    "     \\]\n",
    "   - Construct the corresponding system of equations:\n",
    "     \\[\n",
    "     \\nabla f(x) + \\lambda \\nabla g(x) + \\mu \\nabla h(x) = 0, \\quad g(x) = 0, \\quad h(x) = 0\n",
    "     \\]\n",
    "   - Solve for \\( x_1, x_2, \\lambda, \\mu \\) using `scipy.linalg.solve`.\n",
    "\n",
    "_The correct optimum is the solution that satisfies all constraints and yields the lowest \\( f(x) \\)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLcourse2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
