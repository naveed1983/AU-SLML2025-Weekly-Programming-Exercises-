{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning contents:\n",
    "\n",
    "1. Linear models\n",
    "    - Polynomial function\n",
    "    - Radial basis function\n",
    "    - Sigmoidal basis function\n",
    "    - Optimization of Error function\n",
    "    - Test models\n",
    "2. Bayesian Linear Regression \n",
    "    - Generate data\n",
    "    - Fit the data\n",
    "    - Predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import seaborn as sns; sns.set(); sns.set_palette('bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_func(x): return np.sin(2*np.pi*x)\n",
    "\n",
    "def generate_data(size):\n",
    "    rng = np.random.RandomState(26052605)\n",
    "    x_train = rng.uniform(0., 1., size)\n",
    "    y_train = target_func(x_train) + rng.normal(scale=0.1, size=size)\n",
    "    x_test = np.linspace(0., 1., 100)\n",
    "    y_test = target_func(x_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = generate_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_test, '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the function named linear that takes data point x (scalar), a basis function, and weights (list|ndarray (Mx1) and returns the output of linear basis function model (2nd equation in slide 5 of lecture 11, eq (3.3) in text book). The basis function (Phi(x)) that goes as input to linear will be defined later but it takes x (scalar) and i (index) and returns Phi_i(x) (which is a scalar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, basis, weights):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Polynomial basis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `create_polynomial` below, write the code of `polynomial` function that takes data point `x` (scalar) and `i` (index) and returns the i-th value (scalar) of a polynomial basis function Phi_i(x) (see equation in slide 7 of lecture 11). As you can see, `create_polynomial` returns the resulting function (polynomial)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial():\n",
    "    def polynomial(x, i):\n",
    "        pass\n",
    "    \n",
    "    return polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the different polynomial basis functions for input x ranging from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "n = 11\n",
    "\n",
    "polynomial = create_polynomial()\n",
    "\n",
    "for i in range(n):\n",
    "    y = list(map(lambda x: polynomial(x, i), x))\n",
    "    plt.plot(x, y, '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Radial basis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the create_radial function below, you will implement the inner function radial. The inner function takes two inputs: a data point x (scalar) and an index i. It should return the scalar value of the i-th radial basis function, as defined in slide 8 of Lecture 11.\n",
    "\n",
    "The outer create_radial function takes as input a vector means and a scalar variance.\n",
    "\n",
    "The vector means defines the centers of all the radial basis functions.\n",
    "\n",
    "By passing the index i, the inner radial function selects the i-th mean (means[i]) and uses it, together with the common variance, to compute the value of the radial basis function.\n",
    "\n",
    "Thus, calling radial(x, i) evaluates the i-th basis function at the point x, and by looping over different indices i, you can generate all radial basis functions centered at the values in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_radial(means, variance):\n",
    "    def radial(x, i):\n",
    "        pass\n",
    "    \n",
    "    return radial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below demonstrates how to generate and plot multiple radial basis functions.\n",
    "\n",
    "np.linspace(-1, 1, 100) creates the data points, x, where the functions will be evaluated.\n",
    "\n",
    "np.linspace(-1, 1, 11) defines 11 equally spaced centers (means) for the radial basis functions.\n",
    "\n",
    "create_radial constructs the inner function radial(x, i), which evaluates the i-th radial basis function at point ùë•, using the corresponding mean and a fixed variance.\n",
    "\n",
    "The loop over i calls radial(x, i) for each center and plots all 11 radial basis functions on the same figure.\n",
    "\n",
    "This way you can see how each basis function is shaped and how they overlap across the interval [‚àí1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "n = 11\n",
    "\n",
    "radial = create_radial(np.linspace(-1, 1, 11), 0.1)\n",
    "\n",
    "for i in range(n):\n",
    "    y = list(map(lambda x: radial(x, i), x))\n",
    "    plt.plot(x, y, '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Sigmoidal basis function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the create_sigmoidal function below, you will implement the inner function sigmoidal.\n",
    "\n",
    "The outer function takes as input a vector of means (means) and a slope parameter (s).\n",
    "\n",
    "The inner function sigmoidal(x, i) takes a data point x (scalar) and an index i.\n",
    "\n",
    "It then computes the i-th sigmoidal basis function at point x, using the i-th mean from the vector means and the slope parameter s (see definition in slide 9 of Lecture 11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sigmoidal(means, s):\n",
    "    def sigmoidal(x, i):\n",
    "        pass\n",
    "    \n",
    "    return sigmoidal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like previous case, by looping over different indices i, you can plot all sigmoidal basis functions centered at the values in means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "n = 11\n",
    "\n",
    "sigmoidal = create_sigmoidal(np.linspace(-1, 1, 11), 0.1)\n",
    "\n",
    "for i in range(n):\n",
    "    y = list(map(lambda x: sigmoidal(x, i), x))\n",
    "    plt.plot(x, y, '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Optimization of Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Write the `optimal_weights` function that computes the linear weights for fitting the given data with a chosen basis function (see the last equation on slide 11 of Lecture 11).  \n",
    "\n",
    "**Details & conventions:**  \n",
    "- `basis` is a function of the form `basis(x, i)` that evaluates the i-th basis function at scalar `x`.  \n",
    "- We reserve **i = 0** for the bias term (phi_0(x) = 1).  \n",
    "- For **i = 1 ‚Ä¶ M‚Äì1**, phi_i(x) = basis(x, i).  \n",
    "- Construct the design matrix X with shape (N, M):  \n",
    "  - Column 0 contains all ones (bias).  \n",
    "  - Columns 1 ‚Ä¶ M‚Äì1 contain the basis function values evaluated at the training inputs.  \n",
    "- Use this design matrix together with the target values to compute and return the weight vector `w`.  \n",
    "- The result is a vector of length **M** (one bias + M‚Äì1 basis weights).  \n",
    "\n",
    "**Inputs:**  \n",
    "- `inputs`: list/1D array of training inputs of length N.  \n",
    "- `targets`: list/1D array of training targets of length N.  \n",
    "- `M`: total number of weights (1 bias + M‚Äì1 basis functions).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimial_weights(basis, inputs, targets, M):\n",
    "     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(basis, M, label=''):\n",
    "    plt.plot(x_test, list(map(lambda x: linear(x, basis, optimial_weights(basis, x_train, y_train, M)), x_test)), '-', label=label)\n",
    "    plt.plot(x_train, y_train, 'og')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will test the performance of the optimal model using different basis function on input data generated at the start of the notebook. Which model you believe performs the best on this data? Does the model size affect the model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(create_radial(np.linspace(-1, 1, 10), 0.1), 7, 'radial, M=7')\n",
    "test(create_radial(np.linspace(-1, 1, 10), 0.1), 4, 'radial, M=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(create_polynomial(), 7, 'polynomial, M=7')\n",
    "test(create_polynomial(), 4, 'polynomial, M=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(create_sigmoidal(np.linspace(0, 1, 8), 1), 7, 'sigmoidal, M=7')\n",
    "test(create_sigmoidal(np.linspace(0, 1, 8), 1), 4, 'sigmoidal, M=4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bayesian Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions generate training and test data corresponding to a linear function. The data corresponds to a regression problem where individual input x and output y are both scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_func(x): return -0.5 + 0.3 * x\n",
    "\n",
    "def generate_line_data(size):\n",
    "    rng = np.random.RandomState(26052605)\n",
    "    x_train = rng.uniform(0., 1., size)\n",
    "    y_train = line_func(x_train) + rng.normal(scale=0.05, size=size)\n",
    "    x_test = np.linspace(0., 1., 100)\n",
    "    y_test = line_func(x_test)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "xl_train, yl_train, xl_test, yl_test = generate_line_data(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xl_train, yl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xl_test, yl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Fit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implement `bayesian_regression_fit(PHI, t, alpha, beta)` to return the posterior\n",
    "mean and covariance of the weight vector for Bayesian linear regression (see\n",
    "slide 14 in Lecture 12).\n",
    "\n",
    "Inputs:\n",
    "- PHI: design matrix of shape (N, M), where M is the number of weights\n",
    "  (including the bias term if present).\n",
    "- t: targets, 1D array of length N (or shape (N, 1)).\n",
    "- alpha: scalar prior precision for weights (zero-mean prior).\n",
    "- beta: scalar noise precision (1/sigma^2).\n",
    "\n",
    "Output:\n",
    "- mean: ndarray of shape (M,) or (M,1) ‚Äî posterior mean of the weights.\n",
    "- covariance: ndarray of shape (M, M) ‚Äî posterior covariance of the weights.\n",
    "\n",
    "Notes:\n",
    "- Use a zero-mean Gaussian prior with precision alpha * I_M.\n",
    "- The posterior precision is alpha*I_M + beta * PHI.T @ PHI.\n",
    "- Solve for the posterior mean using the targets and PHI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayessian_regression_fit(PHI, t, alpha, beta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 100\n",
    "\n",
    "x = np.linspace(-1, 1, 100)\n",
    "w0, w1 = np.meshgrid(\n",
    "    np.linspace(-1, 1, 100),\n",
    "    np.linspace(-1, 1, 100)\n",
    ")\n",
    "w = np.array([w0, w1]).transpose(1, 2, 0)\n",
    "\n",
    "basis = create_polynomial()\n",
    "\n",
    "M = 1\n",
    "\n",
    "PHI = np.array([[basis(x[q], i) for q in range(len(x))] for i in range(M + 1)]).T\n",
    "PHI_train = np.array([[basis(xl_train[q], i) for q in range(len(xl_train))] for i in range(M + 1)]).T\n",
    "\n",
    "for begin, end in [[0, 0], [0, 1], [3, 6], [4, 7], [3, 20]]:\n",
    "    \n",
    "    w_mean, w_cov = bayessian_regression_fit(PHI_train[begin:end], yl_train[begin:end], alpha, beta)\n",
    "    \n",
    "    w_sample = np.random.multivariate_normal(\n",
    "        w_mean.reshape(-1), w_cov, size=6\n",
    "    )\n",
    "    y_sample = PHI @ w_sample.T\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(-0.5, 0.3, s=200, marker=\"x\")\n",
    "    plt.contour(w0, w1, multivariate_normal.pdf(w, mean=w_mean, cov=w_cov))\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel(\"$w_0$\")\n",
    "    plt.ylabel(\"$w_1$\")\n",
    "    plt.title(\"prior/posterior\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(xl_train[:end], yl_train[:end], s=100, facecolor=\"none\", edgecolor=\"steelblue\", lw=1)\n",
    "    plt.plot(x, y_sample, c=\"orange\")\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the sample(PHI_train, y_train, alpha, beta, PHI_test) function to compute the posterior predictive distribution for test points in Bayesian linear regression (see slide 16 in Lecture 12).\n",
    "\n",
    "Inputs:\n",
    "\n",
    "PHI_train: design matrix of shape (N, D) for the training set.\n",
    "\n",
    "y_train: targets, array of length N (or shape (N, 1)).\n",
    "\n",
    "alpha: scalar prior precision for the weights.\n",
    "\n",
    "beta: scalar noise precision (1/œÉ¬≤).\n",
    "\n",
    "PHI_test: design matrix of shape (N_test, D) for the test set.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "y: array of length N_test ‚Äî predictive means at the test inputs.\n",
    "\n",
    "y_std: array of length N_test ‚Äî predictive standard deviations at the test inputs.\n",
    "\n",
    "Notes:\n",
    "\n",
    "First, call bayesian_regression_fit with the training data to obtain the posterior mean and covariance of the weights.\n",
    "\n",
    "Compute the predictive mean as\n",
    "y = PHI_test @ w_mean.\n",
    "\n",
    "For each test row œÜ*, compute the predictive variance as\n",
    "(1 / beta) + œÜ*^T @ w_cov @ œÜ*.\n",
    "\n",
    "Return y_std as the square root of these predictive variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(PHI_train, y_train, alpha, beta, PHI_test):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3\n",
    "beta = 2\n",
    "x_train, y_train, x_test, y_test = generate_data(25)\n",
    "\n",
    "basis = create_radial(np.linspace(0, 1, 9), 0.1)\n",
    "\n",
    "M = 8\n",
    "\n",
    "PHI_train = np.array([[basis(x_train[q], i) for q in range(len(x_train))] for i in range(M + 1)]).T\n",
    "PHI_test = np.array([[basis(x_test[q], i) for q in range(len(x_test))] for i in range(M + 1)]).T\n",
    "\n",
    "for begin, end in [[0, 1], [1, 2], [2, 4], [4, 8], [8, 25]]:\n",
    "    \n",
    "    y, y_std = sample(PHI_train[:end], y_train[:end], alpha, beta, PHI_test)\n",
    "    \n",
    "    plt.scatter(x_train[:end], y_train[:end], s=100, facecolor=\"none\", edgecolor=\"steelblue\", lw=2)\n",
    "    plt.plot(x_test, y_test,'b')\n",
    "    plt.plot(x_test, y,'r')\n",
    "    plt.fill_between(x_test, y - y_std, y + y_std, color=\"orange\", alpha=0.5)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
